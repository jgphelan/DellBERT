{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1rowIVDizupUphplJ2yXO_OaJSxA85grR",
      "authorship_tag": "ABX9TyM1iD8vaiBZXS0HFEfaYL0a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgphelan/DellBERT/blob/main/FusionTech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DellBERT\n",
        "\n",
        "A Sentiment and Topic Classifier based on collected costumer reviews.\n",
        "\n",
        "**Make sure to upload and put the .csv file into the sample_data directory!**\n",
        "\n",
        "Name the file \"FusionTech Online Reviews Data Set.csv\"."
      ],
      "metadata": {
        "id": "0yNU-_IxaAlB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment"
      ],
      "metadata": {
        "id": "ksUlLlkeeCYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting weird env issues so just uninstalled a bunch of conflicting packages\n",
        "!pip uninstall -y tsfresh thinc gensim pyLDAvis\n",
        "!pip install -q pandas emoji==2.11.0 unidecode langdetect\n",
        "!pip install --no-cache-dir -q \\\n",
        "      \"numpy==2.0.0\" \\\n",
        "      \"scipy==1.14.0\" \\\n",
        "      \"scikit-learn==1.5.0\"\n",
        "!pip install -q keybert sentence-transformers\n",
        "!pip install -q --upgrade \"transformers>=4.42.0\" \"accelerate>=0.29.0\" \\\n",
        "                            \"peft>=0.10.0\" \"datasets>=2.19.0\" evaluate\n",
        "\n",
        "import numpy as np, scipy, sklearn, pandas as pd, re, emoji, string, gc, torch, platform, transformers\n",
        "from unidecode import unidecode\n",
        "from langdetect import detect, LangDetectException\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from keybert import KeyBERT\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from datasets import Dataset\n",
        "from evaluate import load as load_metric\n",
        "from torch.nn import functional as F\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# make sure we're running a T4 instance\n",
        "print(\"PyTorch CUDA? \", torch.cuda.is_available(),\n",
        "      \"| GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")"
      ],
      "metadata": {
        "id": "QLE0l_5Weh6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading, Pre-Processing, Cleaning\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JXbmA-toT7NS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"sample_data/FusionTech Online Reviews Data Set.csv\")[[\"text\", \"rating\"]] \\\n",
        "       .rename(columns={\"text\":\"review_text\", \"rating\":\"stars\"})\n",
        "\n",
        "# basic filters\n",
        "df.dropna(subset=[\"review_text\", \"stars\"], inplace=True)\n",
        "df.drop_duplicates(subset=\"review_text\", inplace=True)\n",
        "\n",
        "# english filtering\n",
        "\n",
        "def is_english(txt: str) -> bool:\n",
        "    try:\n",
        "        return detect(txt) == \"en\"\n",
        "    except LangDetectException:          # empty / gibberish strings\n",
        "        return False\n",
        "\n",
        "df = df[df[\"review_text\"].apply(is_english)].reset_index(drop=True)\n",
        "\n",
        "# emoji / non-ASCII stripper\n",
        "def normalise(t):\n",
        "    t = unidecode(str(t))                # strips accents\n",
        "    t = emoji.replace_emoji(t, \"\")       # remove emojis\n",
        "    t = t.lower()\n",
        "    t = re.sub(r\"https?://\\S+\", \"\", t)   # URLs\n",
        "    t = re.sub(r\"[^\\w\\s\" + re.escape(string.punctuation) + \"]\", \"\", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "df[\"clean_text\"] = df[\"review_text\"].apply(normalise)\n",
        "df[\"token_cnt\"]  = df[\"clean_text\"].str.split().str.len()\n",
        "df = df[df[\"token_cnt\"].between(5, 512)].reset_index(drop=True)\n",
        "\n",
        "# three-class sentiment labels\n",
        "df[\"sentiment\"] = df[\"stars\"].apply(\n",
        "        lambda x: \"positive\" if x >= 4 else \"negative\" if x <= 2 else \"neutral\"\n",
        ")\n",
        "\n",
        "df[[\"clean_text\",\"sentiment\",\"stars\"]].to_csv(\"clean_reviews.csv\", index=False)\n",
        "print(\"Saved:\", df.shape)\n",
        "del df; gc.collect()"
      ],
      "metadata": {
        "id": "ubiHDySWOChB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EDA"
      ],
      "metadata": {
        "id": "7rWsvHTmbLTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topics"
      ],
      "metadata": {
        "id": "e4Tk-YdnbPyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "df = pd.read_csv(\"clean_reviews.csv\")\n",
        "\n",
        "# extra SW\n",
        "extra_sw = [\"fusiontech\", \"br\", \"amazon\", \"abc\"]\n",
        "# doc term matrix\n",
        "cv = CountVectorizer(lowercase=True, stop_words=list(extra_sw) + [\"english\"], min_df=10, max_df=0.40)\n",
        "\n",
        "dtm = cv.fit_transform(df[\"clean_text\"]) # shape (n_docs, n_terms)\n",
        "vocab = cv.get_feature_names_out()\n",
        "\n",
        "# fit a 10-topic LDA model\n",
        "\n",
        "lda = LatentDirichletAllocation(n_components=10, learning_method=\"batch\", max_iter=20, random_state=42, n_jobs=-1)\n",
        "doc_topic = lda.fit_transform(dtm)\n",
        "\n",
        "# attach dom topic to each review\n",
        "df[\"topic_id\"]   = doc_topic.argmax(axis=1)    # index of highest-prob topic\n",
        "df[\"topic_prob\"] = doc_topic.max(axis=1)       # its probability\n",
        "\n",
        "df.to_csv(\"with_topics.csv\", index=False)\n",
        "print(\"Saved with_topics.csv  â†’\", df.shape, \"rows\")\n",
        "\n",
        "def print_topics(model, vocab, topn=10):\n",
        "    for k, comp in enumerate(model.components_):\n",
        "        words = vocab[np.argsort(comp)[-topn:]][::-1]\n",
        "        print(f\"Topic {k:2d}: {'  '.join(words)}\")\n",
        "\n",
        "print_topics(lda, vocab)\n"
      ],
      "metadata": {
        "id": "dp6ScXi1cxzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Naming Topics based on the LDA above\n",
        "\n",
        "Auto-named each time LDA runs"
      ],
      "metadata": {
        "id": "7t8GpyIUAkit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kw_model = KeyBERT(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# words we never want in the final topic label\n",
        "generic_sw = {\n",
        "    \"gaming\", \"game\", \"laptop\", \"laptops\", \"computer\",\n",
        "    \"fusiontech\", \"amazon\", \"pc\", \"device\", \"andromeda\"\n",
        "}\n",
        "\n",
        "# filler & verb stop-words for cleanup\n",
        "filler   = {\"and\", \"with\", \"has\", \"have\", \"get\", \"the\", \"a\", \"an\"}\n",
        "verb_sw  = {\"run\", \"runs\", \"running\", \"buy\", \"bought\"}\n",
        "\n",
        "# shortens keywords to 2-3\n",
        "def polish(raw: str) -> str:\n",
        "    words = [w for w in raw.lower().split() if w not in filler]\n",
        "    # remove leading verbs\n",
        "    while words and words[0] in verb_sw:\n",
        "        words.pop(0)\n",
        "    # Ensure 2 words\n",
        "    if len(words) < 2:\n",
        "        return \"\"\n",
        "    # swap order for patterns\n",
        "    if len(words) == 2 and words[1].endswith(\"ed\"):\n",
        "        words = words[::-1]\n",
        "    return \" \".join(w.title() for w in words[:3])   # max 3 words\n",
        "\n",
        "vocab   = cv.get_feature_names_out()\n",
        "labels  = {}\n",
        "\n",
        "for k in range(lda.n_components):\n",
        "    # one best-fit review for topic k\n",
        "    best_idx  = doc_topic[:, k].argmax()\n",
        "    best_text = df.loc[best_idx, \"clean_text\"]\n",
        "    # KeyBERT to raw phrase\n",
        "    raw_phrase = kw_model.extract_keywords(\n",
        "                    best_text,\n",
        "                    keyphrase_ngram_range=(1, 3),\n",
        "                    stop_words=list(generic_sw) + [\"english\"],\n",
        "                    top_n=1\n",
        "                 )[0][0]\n",
        "    # polish + fallback\n",
        "    phrase = polish(raw_phrase)\n",
        "    if len(phrase.split()) < 2:\n",
        "        top = [w for w in vocab[lda.components_[k].argsort()[::-1]]\n",
        "               if w not in generic_sw][:2]\n",
        "        phrase = \" \".join(w.title() for w in top)\n",
        "\n",
        "    labels[k] = phrase or f\"Topic {k}\"\n",
        "\n",
        "print(labels)\n",
        "\n",
        "# map into CSV + save\n",
        "df[\"topic_id\"] = df[\"topic_id\"].astype(int).map(labels)\n",
        "df.to_csv(\"with_topics.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "WMRRILN_AoJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aggregate Topics\n",
        "Produces summary .CSV for Customer Service Team"
      ],
      "metadata": {
        "id": "RLwkERv1O5M4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"with_topics.csv\")\n",
        "\n",
        "# mapping for this file (redundant I know, stay with me)\n",
        "def star_to_sentiment(x):\n",
        "    if x >= 4:\n",
        "        return \"positive\"\n",
        "    elif x <= 2:\n",
        "        return \"negative\"\n",
        "    return \"neutral\"\n",
        "\n",
        "df[\"sentiment\"] = df[\"stars\"].apply(star_to_sentiment)\n",
        "\n",
        "# aggregate counts for each topic\n",
        "summary = (\n",
        "    df.groupby(\"topic_id\")[\"sentiment\"]\n",
        "      .value_counts()\n",
        "      .unstack(fill_value=0)                 # columns: negative/neutral/positive\n",
        "      .rename(columns={\n",
        "          \"negative\": \"n_negative\",\n",
        "          \"neutral\" : \"n_neutral\",\n",
        "          \"positive\": \"n_positive\"\n",
        "      })\n",
        "      .assign(total=lambda t: t.sum(axis=1)) # add a total column\n",
        "      .reset_index()\n",
        "      .sort_values(\"n_negative\", ascending=False)  # surface pain-points first\n",
        ")\n",
        "\n",
        "# save new csv\n",
        "summary.to_csv(\"topic_sentiment_summary.csv\", index=False)\n",
        "\n",
        "# test print\n",
        "print(\"\\n\", summary.head(10))"
      ],
      "metadata": {
        "id": "aehSlNFTRE5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis Training\n",
        "\n",
        "This next task involves finetuning BeRT for a sentiment analysis classification task on future unlabeled reviews from a wide variety of sources. It will output a negative, neutral, positive given a review.\n"
      ],
      "metadata": {
        "id": "dTWV57E6bYNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Load and Encode"
      ],
      "metadata": {
        "id": "A9eE1hVdUm57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"clean_reviews.csv\")          # clean_text | sentiment | stars\n",
        "label2id = {\"negative\":0, \"neutral\":1, \"positive\":2}\n",
        "df[\"label\"] = df[\"sentiment\"].map(label2id)\n",
        "\n",
        "# Handle Class imbalance\n",
        "class_counts = df[\"label\"].value_counts().sort_index()   # 0,1,2 order\n",
        "print(class_counts)   # negative / neutral / positive\n",
        "\n",
        "weights = 1.0 / torch.tensor(class_counts, dtype=torch.float)\n",
        "weights = weights / weights.sum() * len(class_counts)    # meanâ‰ˆ1\n",
        "print(\"Class weights:\", weights)"
      ],
      "metadata": {
        "id": "3dls-h32XbIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 Tokenizer etc."
      ],
      "metadata": {
        "id": "sbQBvYEubcaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def encode(batch):\n",
        "    return tok(batch[\"clean_text\"],\n",
        "               truncation=True, padding=\"max_length\", max_length=128)"
      ],
      "metadata": {
        "id": "6hiF3COjXYdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stratified 5-Fold CV with class weighted loss"
      ],
      "metadata": {
        "id": "w3G64w7waopP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skf     = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "metric_acc = load_metric(\"accuracy\")\n",
        "metric_f1w = load_metric(\"f1\")\n",
        "metric_f1m = load_metric(\"f1\")\n",
        "\n",
        "fold_results = []\n",
        "\n",
        "class WeightedLossTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        loss = F.cross_entropy(outputs.logits, labels,\n",
        "                               weight=weights.to(model.device))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(df, df[\"label\"]), 1):\n",
        "    print(f\"\\nâ€”â€” Fold {fold} â€”â€”\")\n",
        "    train_ds = Dataset.from_pandas(df.iloc[train_idx][[\"clean_text\",\"label\"]]).map(encode, batched=True)\n",
        "    val_ds   = Dataset.from_pandas(df.iloc[val_idx][[\"clean_text\",\"label\"]]).map(encode, batched=True)\n",
        "\n",
        "    base = AutoModelForSequenceClassification.from_pretrained(\n",
        "                \"distilbert-base-uncased\", num_labels=3)\n",
        "    base = prepare_model_for_kbit_training(base, use_gradient_checkpointing=True)\n",
        "    model = get_peft_model(base, LoraConfig(\n",
        "                r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "                bias=\"none\", target_modules=[\"q_lin\",\"v_lin\"],\n",
        "                task_type=\"SEQ_CLS\"))\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=f\"chk_fold{fold}\",\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=3,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        fp16=True,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        preds = logits.argmax(-1)\n",
        "        return {\n",
        "            \"accuracy\": metric_acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
        "            \"f1_weighted\": metric_f1w.compute(predictions=preds, references=labels,\n",
        "                                              average=\"weighted\")[\"f1\"],\n",
        "            \"f1_macro\":    metric_f1m.compute(predictions=preds, references=labels,\n",
        "                                              average=\"macro\")[\"f1\"]\n",
        "        }\n",
        "\n",
        "    trainer = WeightedLossTrainer(model=model, args=args,\n",
        "                      train_dataset=train_ds,\n",
        "                      eval_dataset=val_ds,\n",
        "                      compute_metrics=compute_metrics)\n",
        "    trainer.train()\n",
        "    res = trainer.evaluate()\n",
        "    fold_results.append(res)\n",
        "    print(res)"
      ],
      "metadata": {
        "id": "CyRHgpSPavmz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary"
      ],
      "metadata": {
        "id": "FxtHaLwja2hq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv_df = pd.DataFrame(fold_results)\n",
        "print(\"\\nMean across folds:\")\n",
        "print(cv_df[[\"eval_accuracy\",\"eval_f1_weighted\",\"eval_f1_macro\"]].mean())"
      ],
      "metadata": {
        "id": "o7pPVIiAa62X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Training on all data"
      ],
      "metadata": {
        "id": "RNFQHHyFbCQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightedLossTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        loss = F.cross_entropy(outputs.logits, labels,\n",
        "                               weight=weights.to(model.device))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "full_ds = Dataset.from_pandas(df[[\"clean_text\",\"label\"]]).map(encode, batched=True)\n",
        "\n",
        "base = AutoModelForSequenceClassification.from_pretrained(\n",
        "            \"distilbert-base-uncased\", num_labels=3)\n",
        "base = prepare_model_for_kbit_training(base, use_gradient_checkpointing=True)\n",
        "model = get_peft_model(base, LoraConfig(\n",
        "            r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "            bias=\"none\", target_modules=[\"q_lin\",\"v_lin\"],\n",
        "            task_type=\"SEQ_CLS\"))\n",
        "\n",
        "\n",
        "trainer = WeightedLossTrainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"sentiment_final\",\n",
        "        per_device_train_batch_size=16,\n",
        "        num_train_epochs=3,\n",
        "        fp16=True,\n",
        "        save_total_limit=1,\n",
        "        report_to=\"none\"\n",
        "    ),\n",
        "    train_dataset=full_ds\n",
        ")\n",
        "trainer.train()\n",
        "model.save_pretrained(\"sentiment_final\")\n",
        "tok.save_pretrained(\"sentiment_final\")\n",
        "print(\"Final model saved to sentiment_final/\")"
      ],
      "metadata": {
        "id": "1ppP2DdvbE4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nO4X1BKlXpwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo"
      ],
      "metadata": {
        "id": "b3xGOANbQA-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "MODEL_DIR = \"sentiment_final\"\n",
        "BASE_CKPT = \"distilbert-base-uncased\"  # same backbone used during training\n",
        "DEVICE    = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# load tokenzier\n",
        "tok = AutoTokenizer.from_pretrained(BASE_CKPT)\n",
        "\n",
        "# Frozen base model with 3 labels\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    BASE_CKPT,\n",
        "    num_labels=3\n",
        ").to(DEVICE)\n",
        "\n",
        "# LoRA adapter\n",
        "model = PeftModel.from_pretrained(base_model, MODEL_DIR).to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "id2label = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "\n",
        "def predict_sent(texts):\n",
        "    enc = tok(texts, padding=True, truncation=True, max_length=128,\n",
        "              return_tensors=\"pt\").to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        logits = model(**enc).logits\n",
        "    preds = logits.argmax(-1).cpu().tolist()\n",
        "    return [id2label[p] for p in preds]\n",
        "\n",
        "# Demo\n",
        "sample_reviews = [\n",
        "    \"Battery dies after 40 minutes â€“ extremely disappointed.\",\n",
        "    \"Works great for everyday tasks. But then it gets too hot, not terrible though.\",\n",
        "    \"Absolutely love the performance and build quality! 10/10 purchase.\",\n",
        "    \"Keyboard is okay, but the fan noise gets a bit loud under load.\",\n",
        "    \"Arrived DOA: black screen and constant beeping. Had to return it.\"\n",
        "]\n",
        "\n",
        "results = predict_sent(sample_reviews)\n",
        "pd.DataFrame({\"review_text\": sample_reviews,\n",
        "              \"predicted_sentiment\": results})"
      ],
      "metadata": {
        "id": "EwP-omdXRqZd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}